{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB2ACKO0E1y0"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">Identificación de huellas de calzado a partir de imágenes con redes neuronales convolucionales </p>\n",
    "<p style=\"margin: 0; text-align:right;\">MU Ingeniería Computacional y Matemática / Área de Inteligencia Artificial</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Laura Rivera Sanchez</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgKomQZDqBE3"
   },
   "source": [
    "\n",
    "\n",
    "# Identificación de huellas de calzado a partir de imágenes con redes neuronales convolucionales \n",
    "\n",
    "Falta resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDeUR1PzJb6-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awo-goP3wGj6",
    "outputId": "55b30c32-e137-46ac-d5f9-2826b633a8ea"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dFTQYW3wfj_"
   },
   "source": [
    "# Lectura y análisis de los conjuntos de datos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJioTU1k8KR9"
   },
   "source": [
    "## Base de datos 2d FootWear con información de marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-cezL12Twjk"
   },
   "outputs": [],
   "source": [
    "#df_2dfootwear = pd.read_csv('data/2dFootwear/Data-information.csv', delimiter=';')\n",
    "\n",
    "with ZipFile('data/2dFootwear/Part1.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('images')\n",
    "\n",
    "with ZipFile('data/2dFootwear/Part2.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('data/2dFootwear/Part3.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('images')\n",
    "\n",
    "with ZipFile('data/2dFootwear/Part4.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('images')\n",
    "\n",
    "with ZipFile('data/2dFootwear/Part5.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMWhP-7iqoGT"
   },
   "source": [
    "**Análisis de los datos 2d Footwear**\n",
    "(Explicar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM03QnMIRlyZ",
    "outputId": "b70eb7a0-7595-475c-d831-ef3295e1958f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag example: 001_01\n",
      "Nº lines: 150\n",
      "Nº different people: 28\n",
      "Nº of different brands: 60\n",
      "['Adidas' 'Airspeed' 'Airwalk' 'Aldo' 'American Eagle' 'Arizona' 'Asics'\n",
      " 'BAGO' 'BASS' 'Birkenstock' 'Brooks' 'CalvinKlain' 'Champion' 'Clarks'\n",
      " 'Columbus' 'Converse' 'Cooeli' 'Court classic' 'Dansko' 'Deer Stags'\n",
      " 'Dockers' 'Ecco' 'Elcanto' 'Fadedglory' 'Feiyue' 'Fila' 'G.H.Bass&Co'\n",
      " 'Guho' 'HeyBear' 'K-swiss' 'Keen' 'Landya' 'Namuhana' 'Newbalance' 'Nike'\n",
      " 'Ninewest' 'None' 'OP' 'Ofem' 'Prospecs' 'Puma' 'Robin' 'Saucony'\n",
      " 'Shoedy' 'Shoopen' 'Simply vera' 'Skechers' 'Soma' 'Sonoma' 'Sorel'\n",
      " 'Sperry' 'Stone' 'Sugar' 'T2R' 'Teva' 'Truesoft' 'Under Amour' 'Vans'\n",
      " 'Vibram' 'Yonex']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/2dFootwear/Data-information.csv', delimiter=';')\n",
    "df['Brand'] = df['Brand'].str.strip() #eliminar espacios en blanco\n",
    "X_files = df['ID'].values.tolist()\n",
    "brands = df['Brand'].values.tolist()\n",
    "\n",
    "values_brand, counts_brand = np.unique(brands, return_counts=True)\n",
    "print('Tag example:',X_files[0]) #nomenclatura \n",
    "print('Nº lines:',len(X_files)) #lineas en el csv\n",
    "individuals, count_ind = np.unique(df['ID'].str[:3], return_counts=True)\n",
    "print('Nº different people:',len(individuals)) #diferentes individuos de la muestra\n",
    "num_classes = len(values_brand) #se guarda porque será necesario para crear el modelo\n",
    "print('Nº of different brands: %d' %num_classes) #marcas diferentes\n",
    "print(values_brand) #Listado de marcas únicas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6iKuscWnqcw"
   },
   "source": [
    "A continuación, el histograma con el número de muestras según género, en este caso hay 70 muestras de mujeres y 80 de hombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "0uXbUZRSjNlJ",
    "outputId": "95d48de5-2e4b-49ab-acc7-fe197b743f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([71.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 79.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX2cF_1cnitd"
   },
   "source": [
    "Por último, se muestran las cinco marcas que más aparecen en el conjunto de datos. \\\n",
    "Además se seleccionan esas marcas que aparecen minimo 2 veces en la muestra, para crear un subconjunto de datos que elimine las marcas con una única muestra y comprobar en los siguientes casos si influye en el resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "n8Mp-8yejNyz",
    "outputId": "731178a9-f80a-4e27-e05e-159f494c9615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brands with more than 1 register: 20\n",
      "Brands with only 1 register: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='x'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfbrand = pd.DataFrame({'x':values_brand, 'y':counts_brand}) \n",
    "dfbrand = dfbrand.sort_values('y', ascending = False) #ordenar descendientemente\n",
    "\n",
    "\n",
    "#eliminar los que tengan menos de 2 muestras:\n",
    "dfbrandone=dfbrand[dfbrand['y']<2]\n",
    "dfbrand=dfbrand[dfbrand['y']>=2]\n",
    "print('Brands with more than 1 register: %d' %len(dfbrand)) #quedan 20 marcas\n",
    "print('Brands with only 1 register: %d' %len(dfbrandone)) #quedan 20 marcas\n",
    "dfbrand = dfbrand.head(5) # mostrar las 5 marcas que más aparecen en el conjunto de datos\n",
    "dfbrand.plot('x', 'y', kind='bar') #mostrar gráfica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K_8VvbTzxcRe"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_images_to_jpeg(imgPath):\n",
    "  dir_list = os.listdir(\"./\"+imgPath)\n",
    "  result = []\n",
    "\n",
    "  for f in dir_list:\n",
    "\n",
    "    im = Image.open(\"./\"+imgPath+\"/\"+f)\n",
    "    im2=im.resize((400,912))\n",
    "    im3 = im2.crop((40,40,320,872)) #Quitar marco medidor\n",
    "    im3.save(\"./\"+imgPath+\"/\"+f[0:-4]+'jpeg')\n",
    "    \n",
    "    result.append(f[0:-4]+'jpeg')\n",
    "    os.remove(\"./\"+imgPath+\"/\"+f)\n",
    "\n",
    "  print('Nº files:',len(result))\n",
    "  return result\n",
    "\n",
    "def get_images(imgPath):\n",
    "  dir_list = os.listdir(\"./\"+imgPath)\n",
    "  result = []\n",
    "  for f in dir_list:  \n",
    "    result.append(f)\n",
    "\n",
    "\n",
    "  print('Nº files:',len(result))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Smywtw4MVBQA",
    "outputId": "a4800045-7267-4ae2-8ead-e501b6761eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº files: 1500\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all files in /images and convert to jpeg\n",
    "#shoeFiles = get_images_to_jpeg(\"images\") #la primera vez para convertir las imágenes\n",
    "shoeFiles = get_images(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CFzFHyUJxn3"
   },
   "source": [
    "## Base de datos Datos FD-300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvC-lKKtxIwE"
   },
   "outputs": [],
   "source": [
    "#Extracción de los ficheros del zip\n",
    "with ZipFile('data/FID300/images.zip', 'r') as zipObj:\n",
    "  zipObj.extractall('fid300')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lectura de la tabla de resultados\n",
    "df_fid300 = pd.read_csv('data/FID300/label_table.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy-P1CjBwcau"
   },
   "source": [
    "En este caso, las imágenes estan divididas en 3 carpetas:\n",
    "\n",
    "*   *tracks_original*: Conteine las 300 imágenes originales extraidas de la escena del crimen, incluyendo la plantilla de medición.\n",
    "*   *tracks_cropped*: Contiene las imágenes originales pero ya recortadas.\n",
    "*  *references*: Contiene las 1175 imágenes de las huellas de referencia, obtenidas de diferentes calzados.\n",
    "\n",
    "Para el proyecto se decide utilizar las dos últimas categorias, guardandolas en 2 datasets diferenciados: *fid300ref* y *fid300crop*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaodzSQT1RWD"
   },
   "source": [
    "**Análisis de los datos FD-300**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q194Hci3qXNc",
    "outputId": "f46aae71-1e7c-4002-c9b3-f1a43e143527"
   },
   "outputs": [],
   "source": [
    "fid300ref = get_images(\"fid300/references\")\n",
    "fid300crop = get_images(\"fid300/tracks_cropped\")\n",
    "\n",
    "print(len(df_fid300)) #lineas en el csv\n",
    "print(fid300ref[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "mAcb1nLuJDmT",
    "outputId": "a8071680-0767-4a9b-e478-20e58883df14"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_files = df_fid300['X'].values.tolist()\n",
    "y = df_fid300['y'].values.tolist()\n",
    "\n",
    "values_y, counts_y = np.unique(y, return_counts=True)\n",
    "\n",
    "print('Nº of references: %d' %len(values_y))\n",
    "\n",
    "dfref = pd.DataFrame({'x':values_y, 'y':counts_y}) \n",
    "dfref = dfref.sort_values('y', ascending = False) \n",
    "dfref.head(10) # coger los 5 primeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0simRcEVYVq"
   },
   "source": [
    "**Visualización de imágenes**\\\n",
    "Se ha creado la función *plot_image* que permite la visualización de las imágenes de cualquiera de las dos bases de datos.\n",
    "\n",
    "Parámetros:\\\n",
    "*imgPath*: carpeta donde estan las imágenes \\\n",
    "*fileNames*: array con los nombres de los ficheros a mostrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA9G3FNAVaoE"
   },
   "outputs": [],
   "source": [
    "\n",
    "import skimage\n",
    "def plot_image(imgPath, fileNames):\n",
    "  for i in range(len(fileNames)):\n",
    "    filename = fileNames[i]\n",
    "    img = skimage.io.imread(imgPath+filename)\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.title(str(img.shape)+\" , \"+str(img.dtype))\n",
    "    plt.imshow(img)\n",
    "  print(fileNames)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNwsUX5bYEuw"
   },
   "source": [
    "A continuación, se muestra una imagen aleatória de cada uno de los 3 datasets del proyecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(\"images/\",random.choices(shoeFiles,k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "I2IAYZmerA-w",
    "outputId": "5b646a35-84fe-4a8f-828a-f11c93ab7d1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plot_image(\"fid300/references/\",random.choices(fid300ref,k=1))\n",
    "plot_image(\"fid300/tracks_cropped/\",random.choices(fid300crop,k=1))\n",
    "\n",
    "#import cv2 as cv\n",
    "#from google.colab.patches import cv2_imshow \n",
    "#image = cv.imread(\"images/\"+shoeFiles[0]) #leo la primera imagen de train\n",
    "#cv2_imshow(image) #mostrar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNveDqoHnIAq"
   },
   "source": [
    "## División de los datos\n",
    "\n",
    "Para la división de los datos se ha utilizado la funcióon train_test_split dos veces, primero para dividir entre el 80% de train y el 20% de test y después para extraer el equivalente al 10% para la validación. \n",
    "\n",
    "De esta manera se dispone de 70% train, 20% test y 10% validación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de dividir los datos de la base de datos 2d Footwear, se ha formateado la tabla ya que actualmente solo muestra la información de marca según el calzado del usuario, pero no contiene el nombre de la imagen, se ha creado la siguiente función para que el conjunto de datos contenga dos columnas (X:fichero, y:marca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     X         y factor_brand\n",
      "0     021_05_R_03.jpeg  Champion          0.0\n",
      "1     005_10_R_05.jpeg      None          1.0\n",
      "2     026_06_R_01.jpeg      Keen          2.0\n",
      "3     009_08_R_05.jpeg     Asics          3.0\n",
      "4     020_04_R_04.jpeg      None          1.0\n",
      "...                ...       ...          ...\n",
      "1495  017_03_R_01.jpeg      None          1.0\n",
      "1496  018_04_R_02.jpeg    Sperry          7.0\n",
      "1497  012_05_R_03.jpeg  Namuhana         17.0\n",
      "1498  009_12_L_03.jpeg      None          1.0\n",
      "1499  025_03_L_04.jpeg      Nike         10.0\n",
      "\n",
      "[1500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def filesWithBrand(shoeFiles):\n",
    "  files = []\n",
    "  brands = []\n",
    "  for image in shoeFiles:\n",
    "    files.append(image) #filename\n",
    "    person = df[df['ID'].str[:6]==image[:6]]  #persona+contador de calzado\n",
    "    brands.append(person['Brand'].iloc[0])\n",
    "\n",
    "  return pd.DataFrame({'X':files, 'y':brands}) \n",
    "\n",
    "\n",
    "df_shoe_brand = filesWithBrand(shoeFiles)\n",
    "#modificar aquellas marcas que no aparecen mínimo en 2 muestras por \"None\"\n",
    "df_shoe_brand['y'] = df_shoe_brand['y'].replace(dfbrandone['x'].to_numpy(),'None')\n",
    "df_shoe_brand['factor_brand'] = pd.Categorical(pd.factorize(df_shoe_brand.y)[0].astype(np.float32))\n",
    "print(df_shoe_brand)\n",
    "#número de marcas con 2 o más muestras:\n",
    "num_classes = int(max(df_shoe_brand['factor_brand'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lkZuDJGPnG4r"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def  split_datafiles(df):\n",
    "  X_train, X_test = train_test_split(df, test_size=0.2 , random_state=random.randint(0,32),shuffle=True)\n",
    "\n",
    "  X_train, X_val = train_test_split(X_train, test_size=0.14, random_state=random.randint(0,32),shuffle=True) # 0.14 x 0.7 = 0.1\n",
    "  return X_train,  X_test ,X_val\n",
    "\n",
    "#Dividir conjunto de datos:\n",
    "#shoes_train, shoes_test, shoes_val = split_datafiles(df)\n",
    "shoes_train, shoes_test, shoes_val = split_datafiles(df_shoe_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#división de los datos FD300\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5b4bh2pwMAY"
   },
   "source": [
    "# Objetivo 1: Predicción de la marca de calzado \n",
    "\n",
    "Para este apartado se van a utilizar diferentes clasificadores para comparar su resultado.\n",
    "Además de crear una red neuronal para el mismo objetivo, y así podem comparar qué método obtiene mejores resultados.\n",
    "\n",
    "En el caso de la red neuronal, además, se añadiran diferentes procesos para ver si mejoran el modelo:\n",
    "\n",
    "- Augmentación en los datos.\n",
    "- Uso del modelo preentrenado RestNet.\n",
    "- Uso de autokeras para determinar si encuentra mejor modelo que el propuesto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckfXWvNLrxh1"
   },
   "source": [
    "## Extracción de características\n",
    "\n",
    "La extracción de características es el proceso de recuperar los datos más importantes de los datos sin procesar. La extracción de características es encontrar el conjunto de parámetros que definen la forma de una imagen de manera precisa y única.\n",
    "\n",
    "Para la extracción de características se utiliza la técnica Bag of Features, que extrae N características de las imágenes utilizando los descriptores SIFT (Scale Invariant Feature Transform).\n",
    "\n",
    "Existen diferentes algoritmos de extracción de caraterísticas: \n",
    "- **SIFT (Scale Invariant Feature Transform)**\n",
    "- SURF https://docs.opencv.org/3.4/df/dd2/tutorial_py_surf_intro.html, \n",
    "- KAZE, \n",
    "- ORB.\n",
    "\n",
    "Todos ellos estan disponibles en la librería openCV y para este proyecto se escoge utilizar SIFT ya que KAZE y ORB tienen cuenta las rotaciones y en este caso no sería necesario, ya que todas las muestras se toman con la misma metodologia.\n",
    "\n",
    "\n",
    "Aqui nombre diferentes descriptores, además de SIFT:\n",
    "https://www.sciencedirect.com/science/article/pii/S0379073821004461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jrM9PZEjsdQ",
    "outputId": "35b406db-b196-479a-8cde-6bd4cde5fb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python==4.4.0.44 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.4.0.44)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from opencv-contrib-python==4.4.0.44) (1.23.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python==4.4.0.44 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H9CuFUFPqkaO"
   },
   "outputs": [],
   "source": [
    "#extrae y calcula los descriptores SIFT para el conjunto de imágenes enviado\n",
    "def extractSIFT(input_files):\n",
    "    all_features_dict = {}\n",
    "    feature_extractor = cv.SIFT.create()\n",
    "    for i, fname in enumerate(input_files):\n",
    "        rgb = cv.cvtColor(cv.imread(\"images/\"+fname), cv2.COLOR_BGR2RGB)\n",
    "        gray = cv.cvtColor(rgb, cv.COLOR_RGB2GRAY)\n",
    "        kp, desc = feature_extractor.detectAndCompute(gray, None)\n",
    "        all_features_dict[fname] = desc\n",
    "    return all_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lJoAFHphKEKA"
   },
   "outputs": [],
   "source": [
    "#Esta función extrae las características de cada categoria (marca)\n",
    "#input: listado de categorias (marcas), listado de ficheros, marca de cada fichero\n",
    "#output: lista ficheros, lista categorias, lista de características\n",
    "def getFiles(cat_list, X_files, y_values):\n",
    "  all_files = []\n",
    "  all_files_labels = {}\n",
    "  all_features = {}\n",
    "  cat_indexes= []\n",
    "  cat_files = []\n",
    "  cat_features = []\n",
    "\n",
    "  #values_train contiene el listado de categorias sin repeticiones \n",
    "  for cat, label in zip(cat_list, range(len(cat_list))):\n",
    "      \n",
    "      #primero buscar los indices en el listado de cada categoria (dentro bucle)\n",
    "      cat_indexes = [i for i,x in enumerate(y_values) if x == cat]\n",
    "      \n",
    "      #como se saben las posiciones, se cogen esas imagenes de esa categoria:\n",
    "      cat_files = [X_files.iloc[i] for i in cat_indexes]\n",
    "      cat_features = extractSIFT(cat_files)\n",
    "      all_files = all_files + cat_files\n",
    "      all_features.update(cat_features)\n",
    "      for i in cat_files:\n",
    "          all_files_labels[i] = label\n",
    "  return all_files, all_files_labels, all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrFlU5q_Mxjs"
   },
   "source": [
    "Bag of features\n",
    "\n",
    "https://www.researchgate.net/publication/260952140_A_Detailed_Review_of_Feature_Extraction_in_Image_Processing_Systems\n",
    "\n",
    "https://www.sciencedirect.com/topics/engineering/feature-extraction\n",
    "\n",
    "https://www.naun.org/main/NAUN/bio/bio-2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pickle\n",
    "\n",
    "def getFilesBySubset(subset, tipo):\n",
    "    #values_brand= np.unique(subset['y']) #listado marcas únicas que aparecen en subconjunto TRAIN\n",
    "\n",
    "    if os.path.exists(\"saved/all_files_\"+tipo+\".pkl\"):\n",
    "        with open('saved/all_files_'+tipo+'.pkl', 'rb') as fp:\n",
    "            all_files = pickle.load(fp)\n",
    "        with open('saved/all_files_labels_'+tipo+'.pkl', 'rb') as fp:\n",
    "            all_files_labels = pickle.load(fp)\n",
    "        with open('saved/all_features_'+tipo+'.pkl', 'rb') as fp:\n",
    "            all_features = pickle.load(fp)\n",
    "        #all_files_train = np.loadtxt('saved/all_files_train.txt')\n",
    "        #all_files_labels_train = np.loadtxt('saved/all_files_labels_train.txt')\n",
    "        #all_features_train = np.loadtxt('saved/all_features_train.txt')\n",
    "    else:\n",
    "        all_files, all_files_labels, all_features=getFiles(values_brand,subset['X'],subset['y'])\n",
    "\n",
    "    return all_files, all_files_labels, all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UvYVMG1IKvlq"
   },
   "outputs": [],
   "source": [
    "values_brand= np.unique(df_shoe_brand['y']) #listado marcas únicas que aparecen en subconjunto TRAIN\n",
    "\n",
    "all_files_train, all_files_labels_train, all_features_train=getFilesBySubset(shoes_train, \"train\")\n",
    "#Este proceso tarda 40 minutos aproximadamente, por eso se guarda en un fichero para agilizar las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"saved/all_files_train.pkl\"):\n",
    "    with open('saved/all_files_train.pkl', 'wb') as fp:\n",
    "        pickle.dump(all_files_train, fp)\n",
    "    with open('saved/all_files_labels_train.pkl', 'wb') as fp:\n",
    "        pickle.dump(all_files_labels_train, fp)\n",
    "    with open('saved/all_features_train.pkl', 'wb') as fp:\n",
    "        pickle.dump(all_features_train, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación un ejemplo de la matriz de características y la impresión de la imagen con los puntos puntos de interés detectados.\n",
    "\n",
    "TODO: recortar la imagen, ya que los medidores de alrededor no aportan información de marca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oIkl-7H91wPV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#guarda la primera imagen  con los puntos de interés\n",
    "img = cv.imread(\"images/\"+all_files_train[0])\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "features = cv2.SIFT_create()\n",
    "keypoints = features.detect(gray, None)\n",
    "\n",
    "img2=cv.drawKeypoints(gray,keypoints,0,(0,0,255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv.imwrite('results/'+all_files_train[0], img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eggcU01c8AnD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 128)\n",
      "(845, 128)\n"
     ]
    }
   ],
   "source": [
    "#Se crea el Bag of Features con un diccionario de tamaño 75:\n",
    "dictionarySize =75\n",
    "if not os.path.exists(\"saved/bow_dict.pkl\"):\n",
    "    \n",
    "    BOW = cv.BOWKMeansTrainer(dictionarySize)\n",
    "\n",
    "    for feat in all_features_train:\n",
    "        BOW.add(all_features_train[feat])\n",
    "    dictionary = BOW.cluster()\n",
    "else:\n",
    "    with open('saved/bow_dict.pkl', 'rb') as fp:\n",
    "        dictionary = pickle.load(fp)\n",
    "    #dictionary = np.loadtxt('saved/bow_dict.txt')\n",
    "print(dictionary.shape)\n",
    "print(all_features_train[all_files_train[0]].shape) #subdivisión de train: shoes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if not os.path.exists(\"saved/bow_dict.pkl\"):\n",
    "    with open('saved/bow_dict.pkl', 'wb') as fp:\n",
    "        pickle.dump(dictionary, fp)\n",
    "        print('BOW dictionary saved successfully to file')\n",
    "        \n",
    "#Como se trata de un proceso que toma bastante tiempo, también se ha decidido guardar el resultado en un fichero para futuras ejecuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "T4sCiH1J8NDF"
   },
   "outputs": [],
   "source": [
    "def getFeatures(all_files, all_features, all_files_labels):\n",
    "  X = np.empty((len(all_files),dictionarySize))\n",
    "  y = np.empty((len(all_files),))\n",
    "  all_features_BOW = {}\n",
    "\n",
    "  count = 0\n",
    "  for filename in all_files:\n",
    "      desc_query = all_features[filename]\n",
    "      matches = matcher.match(desc_query,dictionary)\n",
    "      train_idxs = []\n",
    "      for j in range(len(matches)):\n",
    "        train_idxs.append(matches[j].trainIdx)\n",
    "      hist, bin_edges = histogram(train_idxs, bins=range(dictionarySize+1))\n",
    "      all_features_BOW[filename] = hist\n",
    "      X[count,:] = hist\n",
    "      y[count] = all_files_labels[filename]\n",
    "      count = count + 1\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LZNg6uMB8RpD"
   },
   "outputs": [],
   "source": [
    "from numpy import histogram\n",
    "import numpy as np\n",
    "\n",
    "#Comentario: A continuación se normaliza el numero de caracteristicas para que todas tengan la misma cantidad,\n",
    "#ya que con el proceso anterior (BOW) es muy probable que las imagenes tengan numero de caracteristicas diferentes.\n",
    "\n",
    "matcher = cv.BFMatcher(normType=cv.NORM_L2)\n",
    "\n",
    "X, y = getFeatures(all_files_train, all_features_train, all_files_labels_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwrFAcxLA-lF"
   },
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "TODO: Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HbNIiDfl8ShT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración kernel=rbf, c= 20 con un score de 0.944530651340996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test_t, y_train, y_test_t = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "kernels =  [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "Cs= [1, 2,3,4, 5, 10, 20]\n",
    "\n",
    "\n",
    "#Se utiliza GridSearchCV para identificar la mejor configuración de entre los diferentes kernel con diferentes\n",
    "#valores de C.\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=dict(C=Cs, kernel=kernels),n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Mejor configuración kernel=%s, c= %s con un score de %s' %(clf.best_estimator_.kernel, clf.best_estimator_.C, clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "o9P4GcpM8brQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612903225806452"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejecuto la configuración con mejor resultado kernel=rbf, c=1:\n",
    "\n",
    "clf_train = svm.SVC(kernel='rbf', C=20).fit(X_train, y_train)\n",
    "clf_train.score(X_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_test, all_files_labels_test, all_features_test=getFilesBySubset(shoes_test, \"test\")\n",
    "X, y = getFeatures(all_files_test, all_features_test, all_files_labels_test)\n",
    "\n",
    "result=clf_train.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766666666666667\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ea8234a0e1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "#Resultado de la predicción:\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, result))\n",
    "print(confusion_matrix(y,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(shoes_test.iloc[3])\n",
    "#print(prediction[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_NBsSq1M9fd"
   },
   "source": [
    "### KNeighborsClassifier\n",
    "\n",
    "TODO: Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=num_classes)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier\n",
    "\n",
    "Explicación\n",
    "TODO: Descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZSrrIGnMuj8"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0us6XzO6MzNQ"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Red neuronal de clasificación de imágenes por Marca (2d Footwear)\n",
    "\n",
    "En este caso los datos ya estan etiquetados, por lo que se puede crear un modelo para que entrene según esa etiqueta. \n",
    "\n",
    "\n",
    "*Es necesario transformar las etiquetas de marcas a numérico para el correcto funcionamiento del modelo. Paso realizado en los primeros pasos*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5qsXNN4PuRbp"
   },
   "outputs": [],
   "source": [
    "#Se ha creado un generador para añadir la aumentación de las imágenes\n",
    "import torchvision.io\n",
    "import torch\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Resize\n",
    "from skimage.io import imread\n",
    "from skimage.util import img_as_float,random_noise\n",
    "from skimage.transform import rotate\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import transform, util\n",
    "\n",
    "\n",
    "def create_variation(theImage,doFlip,doNoise,doRotate):\n",
    "  image = img_as_float(theImage)\n",
    "  if doFlip==True:\n",
    "    image = np.fliplr(image)\n",
    "  if doNoise==True:\n",
    "    image = util.random_noise(image)\n",
    "  if doRotate==True:\n",
    "    image = transform.rotate(image, random.randint(-45, 45),mode='symmetric')\n",
    "  return image\n",
    "\n",
    "class DataGenerator2dFootwear(Sequence):\n",
    "    # Constructor. Input parameters are:\n",
    "    # * fileNames   : List of sample file names\n",
    "    # * doRandomize : If True, the provided file names are shuffled after each training epoch\n",
    "    #                 and each image can be left unchanged, flipped, corrupted with\n",
    "    #                 noise or rotated. 8 possible combinations is chosen randomly with equal probability.\n",
    "    #                 If False, file names are not shuffled and each image is provided unchanged.\n",
    "    # * imgPath     : Path to the images \n",
    "    # * batchSize   : Number of sample images and ground truth items in each batch\n",
    "    def __init__(self,data,doRandomize=False,imgPath='images',batchSize=10):\n",
    "        # Store parameters\n",
    "        self.imgPath=imgPath\n",
    "        self.fileNames=data.copy()\n",
    "        self.batchSize=batchSize\n",
    "        self.doRandomize=doRandomize\n",
    "        # Get number of files (to avoid computing them later)\n",
    "        self.numImages=len(data)\n",
    "        # Shuffle them if required\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Shuffle data if required\n",
    "    def on_epoch_end(self):\n",
    "        if self.doRandomize:\n",
    "            random.shuffle(self.fileNames)\n",
    "\n",
    "    # Returns the number of total batches\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(self.numImages)/float(self.batchSize)))\n",
    "\n",
    "    \n",
    "    # Input  : theIndex - Index of the image to load within self.fileNames.\n",
    "    # Output : theImage - Loaded (and possibly transformed) image. Must be\n",
    "    #                     of float type with values within [0,1]\n",
    "    #          theClass - Shoe brand\n",
    "    def _load_image_(self,theIndex):\n",
    "\n",
    "   \n",
    "        file = self.fileNames[theIndex]  \n",
    "        \n",
    "        img = io.imread(self.imgPath+file)\n",
    "\n",
    "        image = torchvision.io.read_image(self.imgPath+file, torchvision.io.ImageReadMode.GRAY)\n",
    "        #image = torchvision.transforms.CenterCrop(image,[832, 280]) No need\n",
    "        \n",
    "        #scale = T.ConvertImageDtype(torch.float32)\n",
    "        #theImage = scale(img)\n",
    "        theImage = img_as_float(img) #dos maneras de hacer el float32, ¿alguna mejor que la otra?\n",
    "\n",
    "        #añadir aumentación a las imágenes:\n",
    "        if self.doRandomize:\n",
    "          theImage=create_variation(img,random.choice([True, False]),random.choice([True, False]),random.choice([True, False])) \n",
    "        #else:\n",
    "         # theImage=create_variation(img,False, False, False)\n",
    "\n",
    "        #Buscar la imagen en el csv para extraer la Marca:\n",
    "        person = df_shoe_brand[df_shoe_brand['X'].str[:6]==file[:6]]  #persona+contador de calzado\n",
    "        theClass = person['factor_brand'].iloc[0]#self.classes[theIndex] #¿debe ser numérico o podría ser la etiqueta?\n",
    "\n",
    "        return theImage,theClass\n",
    "\n",
    "    # Provides the images,class batch\n",
    "    # Batch format:\n",
    "    # - X : The data. Numpy array of shape (bs,nr,nc,3)\n",
    "    # - y : The ground truth. Numpy array of shape (bs,1)\n",
    "    # Where nb=batch size, nr=num rows, nc=num cols\n",
    "    def __getitem__(self,theIndex):\n",
    "        X=[]\n",
    "        y=[]\n",
    "        bStart=max(theIndex*self.batchSize,0)\n",
    "        bEnd=min((theIndex+1)*self.batchSize,self.numImages)\n",
    "        for i in range(bStart,bEnd):\n",
    "            [curImage,curGT]=self._load_image_(i)\n",
    "            X.append(curImage)\n",
    "            y.append(curGT)\n",
    "        return np.array(X),np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uAv0pkVmQmU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Softmax\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Función que crea el modelo:\n",
    "def create_2dfootwear_model1():\n",
    "    theModel=models.Sequential([\n",
    "        Conv2D(32,(3,3),activation='relu',input_shape=(832, 280,3)), #tamaño imagen reducida antes\n",
    "        MaxPooling2D((2,2),padding='same'),\n",
    "        \n",
    "        Conv2D(32,(3,3),activation='relu'),\n",
    "        MaxPooling2D((2,2),padding='same'),   \n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        \n",
    "\n",
    "        Dense(num_classes,activation='relu'),\n",
    "        Dense(activation='softmax')\n",
    "        Flatten()\n",
    "    ])\n",
    "          \n",
    "\n",
    "\n",
    "    return theModel\n",
    "\n",
    "theModel=create_2d_footwear_model1()\n",
    "theModel.summary()\n",
    "\n",
    "#Este modelo no se utiliza, se utiliza el siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 280, 832, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 280, 832, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 140, 416, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 140, 416, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 70, 208, 32)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 70, 208, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 104, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 232960)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               29819008  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,845,172\n",
      "Trainable params: 29,845,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Modelo utilizado:\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Softmax, Rescaling\n",
    "import tensorflow as tf\n",
    "model_test = models.Sequential([\n",
    "  Rescaling(1./255, input_shape=(280,832,3)),\n",
    "  Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  MaxPooling2D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(num_classes),\n",
    "  Flatten()\n",
    "])\n",
    "\n",
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo mejorado (TODO):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se han testeado diferentes optimizadores, se escoge ADAM.\n",
    "#Optimizador ADAM\n",
    "model_test.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizador SGD\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.01)\n",
    "model_test.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'],optimizer = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop\n",
    "opt = optimizers.RMSprop(learning_rate=0.0001)\n",
    "model_test.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shoes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "DVT_rj-P4iqT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 832, 280, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainGenerator=DataGenerator2dFootwear(shoes_train['X'].tolist(),True, \"images/\")\n",
    "testGenerator=DataGenerator2dFootwear(shoes_test['X'].tolist(),False, \"images/\")\n",
    "valGenerator=DataGenerator2dFootwear(shoes_val['X'].tolist(),False, \"images/\")\n",
    "\n",
    "trainGenerator.__getitem__(0)[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LCF7BxMopt6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 129s 1s/step - loss: 2.5452 - accuracy: 0.2784 - val_loss: 2.5381 - val_accuracy: 0.2544\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 133s 1s/step - loss: 2.5066 - accuracy: 0.2871 - val_loss: 2.5478 - val_accuracy: 0.2544\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 141s 1s/step - loss: 2.4827 - accuracy: 0.2871 - val_loss: 2.5419 - val_accuracy: 0.2544\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 141s 1s/step - loss: 2.4645 - accuracy: 0.2871 - val_loss: 2.5185 - val_accuracy: 0.2544\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 128s 1s/step - loss: 2.4785 - accuracy: 0.2871 - val_loss: 2.5590 - val_accuracy: 0.2544\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 129s 1s/step - loss: 2.4720 - accuracy: 0.2871 - val_loss: 2.5206 - val_accuracy: 0.2544\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 121s 1s/step - loss: 2.4840 - accuracy: 0.2871 - val_loss: 2.5464 - val_accuracy: 0.2544\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 126s 1s/step - loss: 2.4973 - accuracy: 0.2871 - val_loss: 2.5755 - val_accuracy: 0.2544\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 128s 1s/step - loss: 2.4762 - accuracy: 0.2871 - val_loss: 2.5506 - val_accuracy: 0.2544\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 120s 1s/step - loss: 2.4696 - accuracy: 0.2871 - val_loss: 2.5292 - val_accuracy: 0.2544\n"
     ]
    }
   ],
   "source": [
    "trainHistory = model_test.fit(trainGenerator,validation_data=valGenerator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(history):\n",
    "# summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_history_2(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiwElEQVR4nO3de5xU9X3/8debZWFBUBDQCBggES9ELehIvOXnDX+BGhHb1KjBxtbEJMYaa2LVaG62aYymmjY1FjUarcYbMUqiKEjQmBqiq1DlZsAbLKCsCMhtFxY+/eMcdHZZYEb37Aw77+fjsQ/nfM9lPmfUfe/3+z1zjiICMzOzQnUqdQFmZrZrcXCYmVlRHBxmZlYUB4eZmRXFwWFmZkVxcJiZWVEcHGY7IOkXkv6lwG1flzQq65rMSs3BYWZmRXFwmFUASZ1LXYN1HA4O2+WlQ0SXSnpR0jpJP5e0t6TJktZIekJS77ztx0qaI2mVpCclHZS3boSkF9L97gNqWrzXZyTNSvd9RtKhBdZ4iqSZkt6VtFjS91qsPzY93qp0/blpezdJ/ybpDUmrJf0hbTteUl0rn8Oo9PX3JE2UdJekd4FzJY2U9Mf0PZZJ+k9JXfL2/4SkqZLekfSWpG9J+oik9ZL65G13mKR6SdWFnLt1PA4O6yj+GjgZ2B84FZgMfAvoR/Lf+UUAkvYH7gEuTtc9CvxGUpf0l+hDwH8DewIPpMcl3XcEcBvwZaAPMAGYJKlrAfWtA/4W6AWcAnxV0rj0uIPSen+a1jQcmJXu92PgcODotKZ/ArYU+JmcBkxM3/NuYDPwj0Bf4CjgJOCCtIaewBPAY0B/YD9gWkS8CTwJnJF33HOAeyNiU4F1WAfj4LCO4qcR8VZELAGeBv4UETMjogH4NTAi3e5zwCMRMTX9xfdjoBvJL+YjgWrgJxGxKSImAs/lvcf5wISI+FNEbI6IO4DGdL8diognI+KliNgSES+ShNdx6eqzgSci4p70fVdExCxJnYC/B74eEUvS93wmIhoL/Ez+GBEPpe+5ISKej4gZEdEUEa+TBN/WGj4DvBkR/xYRDRGxJiL+lK67AxgPIKkKOIskXK1COTiso3gr7/WGVpZ7pK/7A29sXRERW4DFwIB03ZJofufPN/JeDwK+kQ71rJK0Ctg33W+HJH1S0vR0iGc18BWSv/xJj/FKK7v1JRkqa21dIRa3qGF/Sb+V9GY6fPWvBdQA8DAwTNIQkl7d6oh49gPWZB2Ag8MqzVKSAABAkkh+aS4BlgED0ratPpr3ejHwg4jolffTPSLuKeB9fwlMAvaNiD2A/wK2vs9i4OOt7PM20LCddeuA7nnnUUUyzJWv5a2vbwLmA0MjYneSobz8Gj7WWuFpr+1+kl7HObi3UfEcHFZp7gdOkXRSOrn7DZLhpmeAPwJNwEWSqiX9FTAyb99bgK+kvQdJ2i2d9O5ZwPv2BN6JiAZJI0mGp7a6Gxgl6QxJnSX1kTQ87Q3dBlwvqb+kKklHpXMqfwZq0vevBq4CdjbX0hN4F1gr6UDgq3nrfgvsI+liSV0l9ZT0ybz1dwLnAmNxcFQ8B4dVlIh4meQv55+S/EV/KnBqRGyMiI3AX5H8gnyHZD7kwbx9a4EvAf8JrAQWptsW4gLgaklrgO+QBNjW4y4C/pIkxN4hmRj/i3T1N4GXSOZa3gF+BHSKiNXpMW8l6S2tA5pdZdWKb5IE1hqSELwvr4Y1JMNQpwJvAguAE/LW/w/JpPwLEZE/fGcVSH6Qk5kVQtLvgF9GxK2lrsVKy8FhZjsl6QhgKskczZpS12Ol5aEqM9shSXeQfMfjYoeGgXscZmZWJPc4zMysKBVx47O+ffvG4MGDS12Gmdku5fnnn387Ilp+P6gygmPw4MHU1taWugwzs12KpFYvvfZQlZmZFcXBYWZmRXFwmJlZUSpijqM1mzZtoq6ujoaGhlKXkqmamhoGDhxIdbWfuWNmbaNig6Ouro6ePXsyePBgmt8MteOICFasWEFdXR1DhgwpdTlm1kFU7FBVQ0MDffr06bChASCJPn36dPhelZm1r4oNDqBDh8ZWlXCOZta+Knaoalfy7oZNrN+4+UPtf/2Ul9uwIjPbVXzh6MH06bGzR7UUx8FRIqtWreKXv/wlF1xwwQ63W7+xiTdWrHvvUW5f+9u/4Yc/vZXd99ij4Pda09DET6cv3vmGZtbhjB0+wMHRUaxatYqf/exn2wRHU1MTnTsn/1q2RFC3cgOdqzqx/949qOrUiad/N7Xo95q3phuv/fCUNqnbzMzBUSKXX345r7zyCsOHD6e6upqamhp69+7N/Pnz+fOf/8y4ceN47fVFrN2wgYv+4SIOujB5yufW26esXbuWMWPGcOyxx/LMM88wYMAAHn74Ybp161biMzOzjs7BAXz/N3OYu/TdNj3msP67891TP7Hd9ddccw2zZ89m1qxZPPnkk5xyyinMnj37vctmb5pwC283daFLbOKvRx/P+LPOoE+fPs2OsWDBAu655x5uueUWzjjjDH71q18xfvz4Nj0PM7OWHBxlYuTIke+FRkTwwx/fwOOPTKJrdRWLFy9mwYIF2wTHkCFDGD58OACHH344r7/+ejtXbWaVyMEBO+wZtJfddtvtvdeTHpvK00/9jmlPPc3Afr05/vjjW/0uRteu7094VVVVsWHDhnap1cwqW0V/j6OUevbsyZo12z6Fc9PmLSx682327N2bAX17MX/+fGbMmFGCCs3MWuceR4n06dOHY445hoMPPphu3bqx9957A7B01QaOOW4Uv73vToYNG8YBBxzAkUceWeJqzczeVxHPHM/lctHyQU7z5s3joIMOKlFFrVu9YSNvrFjPR/aoYa+eNW123HI8VzMrf5Kej4hcy3YPVZWJpi1bWLqqgW7VVfRt4y/rmJm1pUyDQ9JoSS9LWijp8lbWXyJprqQXJU2TNChv3bWS5kiaJ+k/lN50SdKT6TFnpT97ZXkO7eXN1Q00bd7CgN7d6OT7S5lZGcssOCRVATcCY4BhwFmShrXYbCaQi4hDgYnAtem+RwPHAIcCBwNHAMfl7ff5iBie/izP6hzay9rGJt5Zt5G+PbrSvYunncysvGXZ4xgJLIyIVyNiI3AvcFr+BhExPSLWp4szgIFbVwE1QBegK1ANvJVhrSWzZUuwZOUGunTuxF67t928hplZVrIMjgFA/p316tK27TkPmAwQEX8EpgPL0p/HI2Je3ra3p8NU39Z27hsu6XxJtZJq6+vrP8x5ZGr5mkYamzYzoFc3qjp5iMrMyl9ZTI5LGg/kgOvS5f2Ag0h6IAOAEyV9Kt388xFxCPCp9Oec1o4ZETdHRC4icv369cv6FD6QDRs3U7+mkd7du9Czxo92NbNdQ5bBsQTYN295YNrWjKRRwJXA2IhoTJtPB2ZExNqIWEvSEzkKICKWpP9cA/ySZEhsl7Ny5UquveHfqeok9tmjuCGqn/zkJ6xfv37nG5qZZSDL4HgOGCppiKQuwJnApPwNJI0AJpCERv4k9yLgOEmdJVWTTIzPS5f7pvtWA58BZmd4Dpl5dcly/vu2W+jfq4bOVcX9a3BwmFkpZXYJT0Q0SboQeByoAm6LiDmSrgZqI2ISydBUD+CBdKpiUUSMJbnC6kTgJZKJ8sci4jeSdgMeT0OjCngCuCWrc8jKxqbNXHXlt6hb9DrHHz2Sk08+mb322ov777+fxsZGTj/9dL7//e+zbt06zjjjDOrq6ti8eTPf/va3eeutt1i6dCknnHACffv2Zfr06aU+HTOrMJle+xkRjwKPtmj7Tt7rUdvZbzPw5Vba1wGHt3GZMPlyePOltj3mRw6BMdds0xwRLFnVwMVXfJdFr7zMrFmzmDJlChMnTuTZZ58lIhg7diy///3vqa+vp3///jzyyCMArF69mj322IPrr7+e6dOn07dv37at2cysAGUxOV5JVm3YxJqGTfTr2ZWt11BNmTKFKVOmMGLECA477DDmz5/PggULOOSQQ5g6dSqXXXYZTz/9NHsU8bhYM7Os+Ntm0GrPIAtNm7ewbNUGunfpTFX3Lu+1RwRXXHEFX/7yNp0sXnjhBR599FGuuuoqTjrpJL7zne9ss42ZWXtyj6MdLVvdwOaAgb27sfvuu793W/VPf/rT3HbbbaxduxaAJUuWsHz5cpYuXUr37t0ZP348l156KS+88AKw/Vuym5m1B/c42smahk2sXL+RvXavoaa6ipq826qPGTOGs88+m6OOOgqAHj16cNddd7Fw4UIuvfRSOnXqRHV1NTfddBMA559/PqNHj6Z///6eHDezdufbqreDzVuCBW+tQRJD9+7R7jcx9G3VzeyD8G3VS+itdxvYuHkLA33nWzPrABwcGVvf2MTbaxvps1tXduvqkUEz2/VVdHBkPUy3JYK6VRuorurER/YozcOZKmEo0szaV8UGR01NDStWrMj0F+vbaxpp2LT1zrft/1FHBCtWrKCmxrdrN7O2U7FjJwMHDqSuro6sbrm+afMWlq9ppFt1FUvWdNn27o7tpKamhoEDB+58QzOzAlVscFRXVzNkyJBMjr1lS3DmLTOYv+xdnvjGcezV03/xm1nHUbFDVVm697nFPPvaO1x1yjCHhpl1OA6ONvbWuw388NF5HP3xPvxNzkNEZtbxODja2Hcens3GzVv419MPYTtPtTUz26U5ONrQY7OX8fict/jHk/dncN/dSl2OmVkmHBxtZPWGTXz74Tl8ov/ufPHYbCbdzczKQcVeVdXWrpk8n3fWbeT2c48o+lGwZma7Ev+GawMzXl3BPc8u4ovHDuHgAX7Ykpl1bA6OD6lh02auePAlPrpndy4etX+pyzEzy5yHqj6kn/5uAa+9vY67zvsk3bpUlbocM7PMucfxIcxd+i4TnnqVzx4+kGOH9i11OWZm7SLT4JA0WtLLkhZKuryV9ZdImivpRUnTJA3KW3etpDmS5kn6D6VfipB0uKSX0mO+197eNm8JLn/wRXp1r+bKv/RDksyscmQWHJKqgBuBMcAw4CxJw1psNhPIRcShwETg2nTfo4FjgEOBg4EjgOPSfW4CvgQMTX9GZ3UOO3L7/7zGi3Wr+e6pn6D3bl1KUYKZWUlk2eMYCSyMiFcjYiNwL3Ba/gYRMT0i1qeLM4Ct9+gIoAboAnQFqoG3JO0D7B4RMyK5H/qdwLgMz6FVi99Zz79N+TMnHbgXnzl0n/Z+ezOzksoyOAYAi/OW69K27TkPmAwQEX8EpgPL0p/HI2Jeun9dIceUdL6kWkm1bXnr9Ijgyodm00nwz+MO9m1FzKzilMXkuKTxQA64Ll3eDziIpAcyADhR0qeKOWZE3BwRuYjI9evXr81qfWjWEn7/53ouG3Mg/Xt1a7PjmpntKrIMjiXAvnnLA9O2ZiSNAq4ExkZEY9p8OjAjItZGxFqSnshR6f75t5xt9ZhZWbG2kat/M5fDPtqL8Z8ctPMdzMw6oCyD4zlgqKQhkroAZwKT8jeQNAKYQBIay/NWLQKOk9RZUjXJxPi8iFgGvCvpyPRqqr8FHs7wHJr559/OZW1jEz/660Pp1MlDVGZWmTILjohoAi4EHgfmAfdHxBxJV0sam252HdADeEDSLElbg2Ui8ArwEvC/wP9GxG/SdRcAtwIL020mZ3UO+aa/vJyHZi3lguP3Y+jePdvjLc3MypKSi5M6tlwuF7W1tR94/3WNTfz/G35Pty5VPHLRsXTt7G+Im1nHJ+n5iMi1bPctRwrw4ykvs3T1BiZ+5SiHhplVvLK4qqqczVy0kl888zrnHDmIwwftWepyzMxKzsGxAxubtnDFgy/xkd1ruPTTB5S6HDOzsuChqh24+fevMP/NNfz8Czl61lSXuhwzs7LgHscOvPluA585dB9OOmjvUpdiZlY23OPYgX8ZdwhNm7eUugwzs7LiHsdO+PnhZmbN+beimZkVxcFhZmZFcXCYmVlRHBxmZlYUB4eZmRXFwWFmZkVxcJiZWVEcHGZmVhQHh5mZFcXBYWZmRXFwmJlZURwcZmZWFAeHmZkVxcFhZmZFyTQ4JI2W9LKkhZIub2X9JZLmSnpR0jRJg9L2EyTNyvtpkDQuXfcLSa/lrRue5TmYmVlzmT3ISVIVcCNwMlAHPCdpUkTMzdtsJpCLiPWSvgpcC3wuIqYDw9Pj7AksBKbk7XdpREzMqnYzM9u+LHscI4GFEfFqRGwE7gVOy98gIqZHxPp0cQYwsJXjfBaYnLedmZmVUJbBMQBYnLdcl7Ztz3nA5FbazwTuadH2g3R46wZJXVs7mKTzJdVKqq2vry+mbjMz24GymByXNB7IAde1aN8HOAR4PK/5CuBA4AhgT+Cy1o4ZETdHRC4icv369cukbjOzSpRlcCwB9s1bHpi2NSNpFHAlMDYiGlusPgP4dURs2toQEcsi0QjcTjIkZmZm7STL4HgOGCppiKQuJENOk/I3kDQCmEASGstbOcZZtBimSnshSBIwDpjd9qWbmdn2ZHZVVUQ0SbqQZJipCrgtIuZIuhqojYhJJENTPYAHkhxgUUSMBZA0mKTH8lSLQ98tqR8gYBbwlazOwczMtqWIKHUNmcvlclFbW1vqMszMdimSno+IXMv2goaqJD0o6RRJZTGZbmZmpVNoEPwMOBtYIOkaSQdkWJOZmZWxgoIjIp6IiM8DhwGvA09IekbS30mqzrJAMzMrLwUPPUnqA5wLfJHkViH/ThIkUzOpzMzMylJBV1VJ+jVwAPDfwKkRsSxddZ8kzzqbmVWQQi/H/Y/0xoPbaG3G3czMOq5Ch6qGSeq1dUFSb0kXZFOSmZmVs0KD40sRsWrrQkSsBL6USUVmZlbWCg2OqvQWH8B7z9rokk1JZmZWzgqd43iMZCJ8Qrr85bTNzMwqTKHBcRlJWHw1XZ4K3JpJRWZmVtYKCo6I2ALclP6YmVkFK/R7HEOBHwLDgJqt7RHxsYzqMjOzMlXo5PjtJL2NJuAE4E7grqyKMjOz8lVocHSLiGkkt2F/IyK+B5ySXVlmZlauCp0cb0xvqb4gfTjTEpIHMJmZWYUptMfxdaA7cBFwODAe+EJWRZmZWfnaaY8j/bLf5yLim8Ba4O8yr8rMzMrWTnscEbEZOLYdajEzs11AoXMcMyVNAh4A1m1tjIgHM6nKzMzKVqHBUQOsAE7MawvAwWFmVmEK/ea45zXMzAwo/Jvjt5P0MJqJiL/fyX6jSR4xWwXcGhHXtFh/CcmjaJuAeuDvI+INSScAN+RteiBwZkQ8JGkIcC/QB3geOCciNhZyHmZm9uEVejnub4FH0p9pwO4kV1htV3o11o3AGJJblZwlaViLzWYCuYg4FJgIXAsQEdMjYnhEDCcZHlsPTEn3+RFwQ0TsB6wEzivwHMzMrA0UOlT1q/xlSfcAf9jJbiOBhRHxarrPvcBpwNy84+Y/jnYGyfdDWvosMDki1qfPBDkRODtddwfwPXzzRTOzdlNoj6OlocBeO9lmALA4b7kubdue84DJrbSfCdyTvu4DrIqIpp0dU9L5kmol1dbX1++kVDMzK1ShcxxraD7H8SbJMzrahKTxQA44rkX7PsAhwOPFHjMibgZuBsjlctvMz5iZ2QdT6FBVzw9w7CXAvnnLA9O2ZiSNAq4EjouIxharzwB+HRGb0uUVQC9JndNeR6vHNDOz7BQ0VCXpdEl75C33kjRuJ7s9BwyVNERSF5Ihp0ktjjsCmACMjYjlrRzjLN4fpiIiAphOMu8Byf2yHi7kHMzMrG0UOsfx3YhYvXUhIlYB393RDmmP4EKSYaZ5wP0RMUfS1ZLGpptdR3KX3QckzUq/nQ6ApMEkPZanWhz6MuASSQtJ5jx+XuA5mJlZGyj0m+OtBcxO942IR4FHW7R9J+/1qB3s+zqtTHynV2mN3Nl7m5lZNgrtcdRKul7Sx9Of60m+fGdmZhWm0OD4B2AjcB/Jt7YbgK9lVZSZmZWvQq+qWgdcnnEtZma2Cyj0qqqpknrlLfeWVPR3K8zMbNdX6FBV3/RKKgAiYiU7/+a4mZl1QIUGxxZJH926kF4q629jm5lVoEIvx70S+IOkpwABnwLOz6wqMzMrW4VOjj8mKUcSFjOBh4ANGdZlZmZlqtCbHH4R+DrJvaFmAUcCf6T5o2TNzKwCFDrH8XXgCOCNiDgBGAGsyqooMzMrX4UGR0NENABI6hoR84EDsivLzMzKVaGT43Xp9zgeAqZKWgm8kVVRZmZWvgqdHD89ffk9SdOBPYDHMqvKzMzKVqE9jvdERMvbnJuZWQX5oM8cNzOzCuXgMDOzojg4zMysKA4OMzMrioPDzMyK4uAwM7OiODjMzKwomQaHpNGSXpa0UNI2j56VdImkuZJelDRN0qC8dR+VNEXSvHSbwWn7LyS9JmlW+jM8y3MwM7PmMgsOSVXAjcAYYBhwlqRhLTabCeQi4lBgInBt3ro7gesi4iBgJLA8b92lETE8/ZmV1TmYmdm2suxxjAQWRsSrEbERuBc4LX+DiJgeEevTxRkkt20nDZjOETE13W5t3nZmZlZCWQbHAGBx3nJd2rY95wGT09f7A6skPShppqTr0h7MVj9Ih7dukNS1tYNJOl9SraTa+vr6D3MeZmaWpywmxyWNB3LAdWlTZ5LH036T5DkgHwPOTdddARyYtu8JXNbaMSPi5ojIRUSuX79+2RVvZlZhsgyOJcC+ecsD07ZmJI0ieab52IhoTJvrgFnpMFcTye3cDwOIiGWRaARuJxkSMzOzdpJlcDwHDJU0RFIX4ExgUv4GkkYAE0hCY3mLfXtJ2tpVOBGYm+6zT/pPAeOA2Rmeg5mZtVD0bdULFRFNki4EHgeqgNsiYo6kq4HaiJhEMjTVA3ggyQEWRcTYiNgs6ZvAtDQgngduSQ99dxooInn++VeyOgczM9uWIqLUNWQul8tFbW1tqcswM9ulSHo+InIt28tictzMzHYdDg4zMyuKg8PMzIri4DAzs6I4OMzMrCgODjMzK4qDw8zMiuLgMDOzojg4zMysKA4OMzMrioPDzMyK4uAwM7OiODjMzKwoDg4zMyuKg8PMzIri4DAzs6I4OMzMrCgODjMzK4qDw8zMiuLgMDOzojg4zMysKJkGh6TRkl6WtFDS5a2sv0TSXEkvSpomaVDeuo9KmiJpXrrN4LR9iKQ/pce8T1KXLM/BzMyayyw4JFUBNwJjgGHAWZKGtdhsJpCLiEOBicC1eevuBK6LiIOAkcDytP1HwA0RsR+wEjgvq3MwM7NtZdnjGAksjIhXI2IjcC9wWv4GETE9ItanizOAgQBpwHSOiKnpdmsjYr0kASeShAzAHcC4DM/BzMxayDI4BgCL85br0rbtOQ+YnL7eH1gl6UFJMyVdl/Zg+gCrIqJpZ8eUdL6kWkm19fX1H+pEzMzsfWUxOS5pPJADrkubOgOfAr4JHAF8DDi3mGNGxM0RkYuIXL9+/dqwWjOzypZlcCwB9s1bHpi2NSNpFHAlMDYiGtPmOmBWOszVBDwEHAasAHpJ6ryjY5qZWXayDI7ngKHpVVBdgDOBSfkbSBoBTCAJjeUt9u0laWtX4URgbkQEMB34bNr+BeDhDM/BzMxayCw40p7ChcDjwDzg/oiYI+lqSWPTza4DegAPSJolaVK672aSYappkl4CBNyS7nMZcImkhSRzHj/P6hzMzGxbSv6I79hyuVzU1taWugwzs12KpOcjIteyvSwmx83MbNfh4DAzs6I4OMzMrCgODjMzK4qDw8zMiuLgMDOzojg4zMysKA4OMzMrioPDzMyK4uAwM7OiODjMzKwoDg4zMyuKg8PMzIri4DAzs6I4OMzMrCgODjMzK4qDw8zMiuLgMDOzojg4zMysKA4OMzMrioPDzMyKkmlwSBot6WVJCyVd3sr6SyTNlfSipGmSBuWt2yxpVvozKa/9F5Jey1s3PMtzMDOz5jpndWBJVcCNwMlAHfCcpEkRMTdvs5lALiLWS/oqcC3wuXTdhogYvp3DXxoREzMq3czMdiDLHsdIYGFEvBoRG4F7gdPyN4iI6RGxPl2cAQzMsB4zM2sDWQbHAGBx3nJd2rY95wGT85ZrJNVKmiFpXIttf5AOb90gqWvblGtmZoUoi8lxSeOBHHBdXvOgiMgBZwM/kfTxtP0K4EDgCGBP4LLtHPP8NHhq6+vrsyvezKzCZBkcS4B985YHpm3NSBoFXAmMjYjGre0RsST956vAk8CIdHlZJBqB20mGxLYRETdHRC4icv369WubMzIzs+wmx4HngKGShpAExpkkvYf3SBoBTABGR8TyvPbewPqIaJTUFziGZOIcSftExDJJAsYBszM7g8mXw5svZXZ4M7NMfeQQGHNNmx82s+CIiCZJFwKPA1XAbRExR9LVQG1ETCIZmuoBPJDkAIsiYixwEDBB0haSXtE1eVdj3S2pHyBgFvCVrM7BzMy2pYgodQ2Zy+VyUVtbW+oyzMx2KZKeT+eamymLyXEzM9t1ODjMzKwoDg4zMyuKg8PMzIri4DAzs6I4OMzMrCgODjMzK4qDw8zMilIRXwCUVA+88QF37wu83Ybl7Or8ebzPn0Vz/jya6wifx6CI2OZmfxURHB+GpNrWvjlZqfx5vM+fRXP+PJrryJ+Hh6rMzKwoDg4zMyuKg2Pnbi51AWXGn8f7/Fk058+juQ77eXiOw8zMiuIeh5mZFcXBYWZmRXFw7ICk0ZJelrRQ0uWlrqdUJO0rabqkuZLmSPp6qWsqB5KqJM2U9NtS11JqknpJmihpvqR5ko4qdU2lIukf0/9PZku6R1JNqWtqaw6O7ZBUBdwIjAGGAWdJGlbaqkqmCfhGRAwDjgS+VsGfRb6vA/NKXUSZ+HfgsYg4EPgLKvRzkTQAuAjIRcTBJI/NPrO0VbU9B8f2jQQWRsSrEbERuBc4rcQ1lURELIuIF9LXa0h+KQwobVWlJWkgcApwa6lrKTVJewD/D/g5QERsjIhVJS2qtDoD3SR1BroDS0tcT5tzcGzfAGBx3nIdFf7LEkDSYGAE8KcSl1JqPwH+CdhS4jrKwRCgHrg9Hbq7VdJupS6qFCJiCfBjYBGwDFgdEVNKW1Xbc3BYwST1AH4FXBwR75a6nlKR9BlgeUQ8X+paykRn4DDgpogYAawDKnJOUFJvkpGJIUB/YDdJ40tbVdtzcGzfEmDfvOWBaVtFklRNEhp3R8SDpa6nxI4Bxkp6nWQI80RJd5W2pJKqA+oiYmsvdCJJkFSiUcBrEVEfEZuAB4GjS1xTm3NwbN9zwFBJQyR1IZngmlTimkpCkkjGr+dFxPWlrqfUIuKKiBgYEYNJ/rv4XUR0uL8qCxURbwKLJR2QNp0EzC1hSaW0CDhSUvf0/5uT6IAXCnQudQHlKiKaJF0IPE5yZcRtETGnxGWVyjHAOcBLkmalbd+KiEdLV5KVmX8A7k7/yHoV+LsS11MSEfEnSROBF0iuRpxJB7z1iG85YmZmRfFQlZmZFcXBYWZmRXFwmJlZURwcZmZWFAeHmZkVxcFhVuYkHe878Fo5cXCYmVlRHBxmbUTSeEnPSpolaUL6vI61km5In88wTVK/dNvhkmZIelHSr9N7HCFpP0lPSPpfSS9I+nh6+B55z7u4O/1WsllJODjM2oCkg4DPAcdExHBgM/B5YDegNiI+ATwFfDfd5U7gsog4FHgpr/1u4MaI+AuSexwtS9tHABeTPBvmYyTf5jcrCd9yxKxtnAQcDjyXdga6ActJbrt+X7rNXcCD6fMrekXEU2n7HcADknoCAyLi1wAR0QCQHu/ZiKhLl2cBg4E/ZH5WZq1wcJi1DQF3RMQVzRqlb7fY7oPe46cx7/Vm/P+ulZCHqszaxjTgs5L2ApC0p6RBJP+PfTbd5mzgDxGxGlgp6VNp+znAU+nTFeskjUuP0VVS9/Y8CbNC+K8WszYQEXMlXQVMkdQJ2AR8jeShRiPTdctJ5kEAvgD8VxoM+XeTPQeYIOnq9Bh/046nYVYQ3x3XLEOS1kZEj1LXYdaWPFRlZmZFcY/DzMyK4h6HmZkVxcFhZmZFcXCYmVlRHBxmZlYUB4eZmRXl/wCTwupMk5caxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(trainHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 8s 258ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "predicted_y = model_test.predict(testGenerator)\n",
    "# Evaluate the best model with testing data.\n",
    "#print(confusion_matrix(shoes_test['y'],predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de autokeras\n",
    "La librería autokeras se encarga de buscar la combinación de Layers del modelo que puede obtener mejor resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autokeras in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autokeras) (2.11.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autokeras) (21.3)\n",
      "Requirement already satisfied: keras-nlp>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autokeras) (0.4.1)\n",
      "Requirement already satisfied: keras-tuner>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autokeras) (1.3.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autokeras) (1.0.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from keras-nlp>=0.4.0->autokeras) (1.23.4)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from keras-nlp>=0.4.0->autokeras) (1.4.0)\n",
      "Requirement already satisfied: ipython in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from keras-tuner>=1.1.0->autokeras) (7.13.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from keras-tuner>=1.1.0->autokeras) (2.23.0)\n",
      "Requirement already satisfied: kt-legacy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from keras-tuner>=1.1.0->autokeras) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (41.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (1.51.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (15.0.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (2.11.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (23.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (3.3.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow>=2.8.0->autokeras) (2.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from packaging->autokeras) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->autokeras) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->autokeras) (2020.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->autokeras) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.2.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.16.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->keras-tuner>=1.1.0->autokeras) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->keras-tuner>=1.1.0->autokeras) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->keras-tuner>=1.1.0->autokeras) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->keras-tuner>=1.1.0->autokeras) (3.0.4)\n",
      "Requirement already satisfied: appnope in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.1.0)\n",
      "Requirement already satisfied: pexpect in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (3.0.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.17.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.3.3)\n",
      "Requirement already satisfied: pygments in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.7.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jedi>=0.10->ipython->keras-tuner>=1.1.0->autokeras) (0.7.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (6.0.0)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner>=1.1.0->autokeras) (0.1.9)\n",
      "Requirement already satisfied: ipython-genutils in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from traitlets>=4.2->ipython->keras-tuner>=1.1.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (2.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pexpect->ipython->keras-tuner>=1.1.0->autokeras) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.8.0->autokeras) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#autokeras\n",
    "\n",
    "!pip install autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 832, 280, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "batch_size=10\n",
    "\n",
    "train3_loader = torch.utils.data.DataLoader(DataGenerator2dFootwear(shoes_train['X'].tolist(),True, \"images/\"))\n",
    "\n",
    "test3_loader = torch.utils.data.DataLoader(DataGenerator2dFootwear(shoes_test['X'].tolist(),False, \"images/\"))\n",
    "\n",
    "ex_train = enumerate(train3_loader)\n",
    "train_batch_idx, (train_data, train_annotation) = next(ex_train)\n",
    "\n",
    "ex_test = enumerate(test3_loader)\n",
    "test_batch_idx, (test_data, test_annotation) = next(ex_test)\n",
    "\n",
    "\n",
    "#test_data = torch.moveaxis (test_data, 1, -1)\n",
    "#train_data = torch.moveaxis (train_data, 1, -1)\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices ((train_data, train_annotation))\n",
    "test_set = tf.data.Dataset.from_tensor_slices ((test_data, test_annotation))\n",
    "\n",
    "\n",
    "trainGenerator=DataGenerator2dFootwear(shoes_train['X'].tolist(),True, \"images/\")\n",
    "testGenerator=DataGenerator2dFootwear(shoes_test['X'].tolist(),False, \"images/\")\n",
    "valGenerator=DataGenerator2dFootwear(shoes_val['X'].tolist(),False, \"images/\")\n",
    "\n",
    "trainGenerator.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 832, 280, 3)\n"
     ]
    }
   ],
   "source": [
    "#https://autokeras.com/image_classifier/\n",
    "#https://medium.com/analytics-vidhya/how-to-use-autokeras-to-build-image-classification-models-using-one-line-of-code-c35b0c36e66e\n",
    "\n",
    "images_train = np.empty([280,832], dtype=int)\n",
    "\n",
    "X = np.array([io.imread(\"images/\"+p) for p in shoes_train.X.values])\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ./image_classifier/tuner0.json\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 323s 10s/step - loss: 36.8848 - accuracy: 0.1406\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 370s 11s/step - loss: 2.4328 - accuracy: 0.3356\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 344s 10s/step - loss: 1.5474 - accuracy: 0.5461\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 384s 12s/step - loss: 0.3066 - accuracy: 0.9612\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 407s 12s/step - loss: 0.0261 - accuracy: 0.9971\n",
      "Epoch 6/10\n",
      "31/33 [===========================>..] - ETA: 22s - loss: 0.0052 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "clf = ak.ImageClassifier( max_trials=1,loss = 'binary_crossentropy',\n",
    "    metrics = 'accuracy')\n",
    "\n",
    "# Feed the tensorflow Dataset to the classifier.\n",
    "autohistory = clf.fit(X, shoes_train['y'],epochs=10, validation_split = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.export_model()\n",
    "\n",
    "# Para ver la información de la función summary() que incluye las capas del mejor modelo\n",
    "model.summary()\n",
    "\n",
    "#with open('saved/auto_model.pkl', 'wb') as fp:\n",
    "#        pickle.dump(model, fp)\n",
    "#        print('Model saved successfully to file')\n",
    "\n",
    "#print(autohistory.history)\n",
    "plot_history_2(autohistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the best model.\n",
    "test = np.empty([280,832], dtype=int)\n",
    "\n",
    "test = np.array([io.imread(\"images/\"+p) for p in shoes_test.X.values])\n",
    "\n",
    "predicted_y = clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "print(confusion_matrix(shoes_test['y'],predicted_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(test, shoes_test['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene una eficacia del 38%, bastante baja respecto a la de entrenamiento.\n",
    "\n",
    "¿Esto implica sobreentrenamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de modelo pre-entrenado ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "import ssl\n",
    "#se usa para evitar el error SSL que lanza VGG16\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#vgg16 por defecto usa ImageNet:\n",
    "modelPre = models.Sequential()\n",
    "modelPre.add(VGG16(include_top=False, input_shape=(832, 280, 3)))\n",
    "modelPre.add(Flatten())\n",
    "modelPre.add(Dense(num_classes))\n",
    "modelPre.summary()\n",
    "\n",
    "#inp = Input(shape= (320,872, 3))\n",
    "#out = VGG16(weights='imagenet')(inp)\n",
    "#modelPre = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "modelPre.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preHistory = modelPre.fit(trainGenerator,validation_data=valGenerator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(preHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de datos\n",
    "\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-Aite4h109M"
   },
   "source": [
    "# Objetivo 2: Búsqueda de similitud (FD300) NOT TESTED\n",
    "\n",
    "método No supervisado. La idea es crear dos CNN: una codifica las imagenes y la otra las decodifica.\n",
    "La idea es crear dos CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class FolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch dataset from folder, returning two tensor images.\n",
    "    Args: \n",
    "    main_dir : directory where images are stored.\n",
    "    transform (optional) : torchvision transforms to be applied while making dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, main_dir, transform=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs = os.listdir(main_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            tensor_image = self.transform(image)\n",
    "\n",
    "        return tensor_image, tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Codifica\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodifica\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Upscale the image with convtranspose etc.\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        x = self.relu5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQxd-ubr18By"
   },
   "source": [
    "# Analisis de resultados\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
